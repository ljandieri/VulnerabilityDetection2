flair:
- is it better to class-predict a big array of sentences (about 70k) or individually? or maybe in smaller batches (say 1000?)
- using 'distilbert-base-uncased' transformer model; what other models are there? can I change internal structure of transformer blocks and model layers?
- what is label_type? can I set it to custom one ('vulnerability') ?
- what is recommended learning rate for textClassifier?
- what other parameters are available for training model? is there a list of them ?
2022-03-16 23:24:58,279  - learning_rate: "5e-05"
2022-03-16 23:24:58,279  - mini_batch_size: "32"
2022-03-16 23:24:58,279  - patience: "3"
2022-03-16 23:24:58,279  - anneal_factor: "0.5"
2022-03-16 23:24:58,279  - max_epochs: "5"
2022-03-16 23:24:58,279  - shuffle: "True"
2022-03-16 23:24:58,279  - train_with_dev: "False"
2022-03-16 23:24:58,279  - batch_growth_annealing: "False"

- should I try flair LSTM instead of transformers?

char2vec LSTM:
- implement custom char embedding for char2vec? (now using gensim, but adjusting it for chars)
- could I simply use character uni-codes ( ord(c) ) instead of c2v vectors?
TODO:
- train char2vec models with higher vectorsize (on HU computers)
- try one-hot embeddings instead of char2vec with 'unk'= ~10 (charMap should not be too big, so one-hot can work)


transformers (keras):
- char-lvl transformers?
- keras model early-stopping & keras optimizers
TODO:
- any additional filters for keras tokenizer (now using '\n'), probably should add '\r'


TODO:
+ implement custom class for accuracy, precision, recall, f1 scores.
+ huggingface transformers (distilBERT)
- code2vec
- vary step and window size for generating allblocks from datasets
- train models on entire dataset (all vulnerabilities) and evaluate performance

MAYBE:
- zero-shot classification with flair TARS
- bi-directional LSTM (keras)
- switch transformer model (keras)
- openAI GPT-3
- Character-level recurrent sequence-to-sequence model
- flair roberta
-- train a model on every dataset (SQL, XSS, XSRF, etc.) and see how it performs
- XLNet transformer

EXTRA:
- use seq2seq models to 'translate' (fix) vulnerable code into code that is not vulnerable

----------------------------------------------------
- Do additional hyperparameter tests + on every vulnerability type to have more comprehensive data
- Train Transformer Tokenizer on
- Hyperparamters for C2V model
-