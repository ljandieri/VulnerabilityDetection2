import gensim.utils
import torch
from myutils import getNewName
import myutils
import sys
import os.path
import json
from datetime import datetime
import random
import pickle
import numpy
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.preprocessing import sequence
from keras import backend as K
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
import os
from sklearn.utils import class_weight
import tensorflow as tf
from gensim.models import Word2Vec, KeyedVectors
from gensim import models
import torch.nn as nn
import transformerBlockModel
from transformerBlockModel import TokenAndPositionEmbedding
from transformerBlockModel import TransformerBlock
# for automatically saving results
auto_trace = True
trace_file = None

#default mode / type of vulnerability
mode = "remote_code_execution"

#get the vulnerability from the command line argument
if (len(sys.argv) > 1):
  mode = sys.argv[1]
embed_dim = 32  # [1,2,3,176,200,225]
num_heads = 5  # Number of attention heads [0,1,2,5,10,15,20,30,40,50]
ff_dim = 64  # Hidden layer size in feed forward network inside transformer
# ff_dim = [4,8,16,32,64,96,128,178,256,375,512]
dropout = 0.1 # [0, 0.001, 0.005, 0.01, 0.1, 0.15, 0.2, 0.25, 0.4, 0.5, 0.6, 0.8 ]
output_size = 1 # Size of final output from transformer model (prediction size, default=1)
optimizer = 'adam' # SGD,RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl, Adam
batch_size = 1024 # [256, 512, 1024]
epochs = 100
tokenizer_vocab_size = 30000 # vocab size for the keras tokenizer

epochTest = [2,4,5,10,25,50,100,150,200,300,500,750,1000,2,4,5,10,25,50,100,150,200,300,500,750,1000]

for testParam in epochTest:
    epochs = testParam
    print(f'Now testing the parameter epoch for value: {testParam}')
    progress = 0
    count = 0


    ### paramters for the filtering and creation of samples
    restriction = [20000,5,6,10] #which samples to filter out
    step = 5 #step lenght n in the description
    fulllength = 200 #context length m in the description




    params = [step,fulllength,embed_dim,num_heads,ff_dim,dropout,output_size,optimizer,batch_size,epochs,tokenizer_vocab_size]
    paramNames = ['step','length','edim','numhead','ffsz','droput','outsz','opt_','btsz','eps','tokvocsize']
    paramsZipped = [el[0] + str(el[1]) for el in list(zip(paramNames,params))]
    modelName = 'Transformer_model_filtertest_'+ '_'.join(paramsZipped) +'_'+mode +'.h5'

    thisDir = os.getcwd()
    modelDir = 'transformers\\transformerModels\\' + mode + '\\'
    modelRelativePath = 'transformers\\transformerModels\\' + mode
    if not os.path.isdir(modelRelativePath):
        os.mkdir(modelRelativePath)
    #modelName = getNewName(modelDir, 'Transformer_model_edim32_numhead2_ffsz32_droput0.1_outsz1_opt_adam_btsz1024_eps100_tokvocsize30000_remote_code_execution.h5')
    #"""
    #load data
    thisDataPath = thisDir + '/' + 'data/plain_' + mode

    with open('data/plain_' + mode, 'r') as infile:
      data = json.load(infile)
    #"""

    now = datetime.now() # current date and time
    nowformat = now.strftime("%H:%M")
    print("finished loading. ", nowformat)
    """
    allblocks = None
    with open(f'dataProcessed\\{mode}_processed_step{step}_length{fulllength}', 'rb') as fp:
      allblocks = pickle.load(fp)
    """
    #"""
    allblocks = []
    for r in data:
      progress = progress + 1

      for c in data[r]:

        if "files" in data[r][c]:
        #  if len(data[r][c]["files"]) > restriction[3]:
            #too many files
        #    continue

          for f in data[r][c]["files"]:

      #      if len(data[r][c]["files"][f]["changes"]) >= restriction[2]:
              #too many changes in a single file
       #       continue

            if not "source" in data[r][c]["files"][f]:
              #no sourcecode
              continue

            if "source" in data[r][c]["files"][f]:
              sourcecode = data[r][c]["files"][f]["source"]
         #     if len(sourcecode) > restriction[0]:
                #sourcecode is too long
         #       continue

              allbadparts = []

              for change in data[r][c]["files"][f]["changes"]:

                    #get the modified or removed parts from each change that happened in the commit
                    badparts = change["badparts"]
                    count = count + len(badparts)

               #     if len(badparts) > restriction[1]:
                      #too many modifications in one change
               #       break

                    for bad in badparts:
                      #check if they can be found within the file
                      pos = myutils.findposition(bad,sourcecode)
                      if not -1 in pos:
                          allbadparts.append(bad)

                 #   if (len(allbadparts) > restriction[2]):
                      #too many bad positions in the file
                 #     break

              if(len(allbadparts) > 0):
             #   if len(allbadparts) < j[2]:
                  #find the positions of all modified parts
                  positions = myutils.findpositions(allbadparts,sourcecode)

                  #get the file split up in samples
                  blocks = myutils.getblocks(sourcecode, positions, step, fulllength)

                  for b in blocks:
                      #each is a tuple of code and label
                      allblocks.append(b)
    #"""

    keys = []

    #randomize the sample and split into train, validate and final test set
    print("total length of the sample set: " + str(len(allblocks)))
    for i in range(len(allblocks)):
      keys.append(i)
    random.shuffle(keys)

    cutoff = round(0.7 * len(keys)) #     70% for the training set
    cutoff2 = round(0.85 * len(keys)) #   15% for the validation set and 15% for the final test set

    keystrain = keys[:cutoff]
    keystest = keys[cutoff:cutoff2]
    keysfinaltest = keys[cutoff2:]

    print("cutoff " + str(cutoff))
    print("cutoff2 " + str(cutoff2))

    TrainX = []
    TrainY = []
    ValidateX = []
    ValidateY = []
    FinaltestX = []
    FinaltestY = []

    print("Creating training dataset... (" + mode + ")")

    for k in keystrain:
      block = allblocks[k]
      code = block[0]
      TrainX.append(code)
      TrainY.append(block[1]) #append the label to the Y (dependent variable)

    print("Creating validation dataset...")
    for k in keystest:
      block = allblocks[k]
      code = block[0]
      ValidateX.append(code)
      ValidateY.append(block[1]) #append the label to the Y (dependent variable)

    print("Creating finaltest dataset...")
    for k in keysfinaltest:
      block = allblocks[k]
      code = block[0]
      FinaltestX.append(code)
      FinaltestY.append(block[1]) #append the label to the Y (dependent variable)

    print("Train length: " + str(len(TrainX)))
    print("Test length: " + str(len(ValidateX)))
    print("Finaltesting length: " + str(len(FinaltestX)))
    now = datetime.now() # current date and time
    nowformat = now.strftime("%H:%M")
    print("time: ", nowformat)

    filters = "\t\n"
    tokenizer = keras.preprocessing.text.Tokenizer(num_words=tokenizer_vocab_size,filters=filters)
    all_code = []
    all_code.extend(TrainX)
    all_code.extend(ValidateX)
    all_code.extend(FinaltestX)
    tokenizer.fit_on_texts(TrainX)

    train_text_to_int = tokenizer.texts_to_sequences(TrainX)
    val_texts_to_int = tokenizer.texts_to_sequences(ValidateX)
    test_texts_to_int = tokenizer.texts_to_sequences(FinaltestX)

    maxlen = len(train_text_to_int[0])

    for i in range(len(train_text_to_int)):
        if len(train_text_to_int[i]) > maxlen: maxlen = len(train_text_to_int[i])

    for i in range(len(val_texts_to_int)):
        if len(val_texts_to_int[i]) > maxlen: maxlen = len(val_texts_to_int[i])

    for i in range(len(test_texts_to_int)):
        if len(test_texts_to_int[i]) > maxlen: maxlen = len(test_texts_to_int[i])

    print(f'Maxlen has been calculated to be {maxlen}')

    ## pad sequences
    padding_type = 'post'
    X_train = keras.preprocessing.sequence.pad_sequences(train_text_to_int,
                                                                  maxlen=maxlen,
                                                                  truncating='pre',
                                                                  padding=padding_type)

    y_train = numpy.array(TrainY)
    X_test = keras.preprocessing.sequence.pad_sequences(val_texts_to_int,
                                                                  maxlen=maxlen,
                                                                  truncating='pre',
                                                                  padding=padding_type)
    y_test =  numpy.array(ValidateY)
    X_finaltest = keras.preprocessing.sequence.pad_sequences(test_texts_to_int,
                                                                  maxlen=maxlen,
                                                                  truncating='pre',
                                                                  padding=padding_type)
    y_finaltest = numpy.array(FinaltestY)

    #in the original collection of data, the 0 and 1 were used the other way round, so now they are switched so that "1" means vulnerable and "0" means clean.

    for i in range(len(y_train)):
      y_train[i] = 1 - y_train[i]

    for i in range(len(y_test)):
      y_test[i] = 1 - y_test[i]

    for i in range(len(y_finaltest)):
      y_finaltest[i] = 1 - y_finaltest[i]


    now = datetime.now() # current date and time
    nowformat = now.strftime("%H:%M")
    print("numpy array done. ", nowformat)

    print(str(len(X_train)) + " samples in the training set.")
    print(str(len(X_test)) + " samples in the validation set.")
    print(str(len(X_finaltest)) + " samples in the final test set.")

    csum = 0
    for a in y_train:
      csum = csum+a
    print("percentage of vulnerable samples: "  + str(int((csum / len(X_train)) * 10000)/100) + "%")

    testvul = 0
    for y in y_test:
      if y == 1:
        testvul = testvul+1
    print("absolute amount of vulnerable samples in test set: " + str(testvul))


    now = datetime.now() # current date and time
    nowformat = now.strftime("%H:%M")
    print("Starting Transformer: ", nowformat)

    # creating the model
    model = None

    model = keras.Sequential()
    model.add(layers.Input(shape=(maxlen, )))
    model.add(TokenAndPositionEmbedding(maxlen, tokenizer_vocab_size, embed_dim))
    model.add(TransformerBlock(embed_dim, num_heads, ff_dim))
    model.add(layers.GlobalAveragePooling1D())
    model.add(layers.Dropout(dropout))
    model.add(layers.Dense(ff_dim, activation='relu'))
    model.add(layers.Dropout(dropout))
    model.add(layers.Dense(output_size, activation='sigmoid'))
    #config = model.get_config()

    #model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    model.compile(loss=myutils.f1_loss, optimizer=optimizer, metrics=[myutils.f1])

    history = model.fit(X_train.astype(float), y_train.astype(float), batch_size=batch_size, epochs=epochs)
    #history = model.fit(X_train.astype(float), y_train.astype(float), batch_size=2048, epochs=100, validation_data=(X_test.astype(float), y_test.astype(float)))

    now = datetime.now() # current date and time
    nowformat = now.strftime("%H:%M")
    print("Compiled Transformer: ", nowformat)

    split = modelName.split(mode)
    modelName = split[0] + 'mxl' + str(maxlen) + '_' + mode +split[1]
    modelName = getNewName(modelDir,modelName)

    if auto_trace:
        traceDir = 'transformers/transformerTraces/' + mode
        if not os.path.isdir(traceDir):
            os.mkdir(traceDir)
        consoleOutputDir = 'transformers/transformerConsoleOutput/' + mode
        if not os.path.isdir(consoleOutputDir):
            os.mkdir(consoleOutputDir)
        trace_path = traceDir + '/' + getNewName(traceDir,modelName + '.txt')
        trace_file = open(trace_path, 'w')
        print(trace_path)
        consoleOutputPath = consoleOutputDir + '/' + getNewName(consoleOutputDir,modelName + '.txt')
        consoleOutputFile = open(consoleOutputPath, 'w')

    #validate data on train and test set

    for dataset in ["train","test","finaltest"]:
        print("Now predicting on " + dataset + " set ")

        ## placeholder values for variable declaration
        accuracy = -13
        precision = -13
        recall = -13
        F1Score = -13

        if dataset == "train":
            yhat_classes = (model.predict(X_train) > 0.5).astype("int32")
            accuracy = accuracy_score(y_train, yhat_classes)
            precision = precision_score(y_train, yhat_classes)
            recall = recall_score(y_train, yhat_classes)
            F1Score = f1_score(y_train, yhat_classes)

        if dataset == "test":
            yhat_classes = (model.predict(X_test) > 0.5).astype("int32")
            accuracy = accuracy_score(y_test, yhat_classes)
            precision = precision_score(y_test, yhat_classes)
            recall = recall_score(y_test, yhat_classes)
            F1Score = f1_score(y_test, yhat_classes)


        if dataset == "finaltest":
            yhat_classes = (model.predict(X_finaltest) > 0.5).astype("int32")
            accuracy = accuracy_score(y_finaltest, yhat_classes)
            precision = precision_score(y_finaltest, yhat_classes)
            recall = recall_score(y_finaltest, yhat_classes)
            F1Score = f1_score(y_finaltest, yhat_classes)

        thisPrint = '\n'.join([
          "Accuracy: " + str(accuracy),
          "Precision: " + str(precision),
          "Recall: " + str(recall),
          'F1 score: %f' % F1Score,
          "\n"
        ])
        print(thisPrint)
        trace_file.write(thisPrint)
        consoleOutputFile.write(thisPrint)

    if auto_trace:
        trace_file.close()
        consoleOutputFile.close()

    # save the transformer final test sets
    with open(modelDir + '_dataset_finaltest_X', 'wb') as fp:
      pickle.dump(X_finaltest, fp)
    with open(modelDir + '_dataset_finaltest_Y', 'wb') as fp:
      pickle.dump(y_finaltest, fp)

    now = datetime.now() # current date and time
    nowformat = now.strftime("%H:%M")
    print("saving transformer model " + mode + ". ", nowformat)
    model.save(modelDir + modelName)
    print("\n\n")

os.system('shine.mp3')