import sys
import subprocess
import time
import torch
import gensim
from gensim.models import Word2Vec
import nltk
import os.path
import pickle
import sys

nltk.download('punkt')
all_words = []

mode = "withString"  # default
if (len(sys.argv) > 1):
    mode = sys.argv[1]

# Loading the training corpus
print("Loading " + mode)
"""
with open('w2v/pythontraining' + '_' + mode + "_X", 'r') as file:
    pythondata = file.read().lower().replace('\n', ' ')
"""

with open('w2v/pythontraining' + '_' + mode + "_X", 'r') as file:
    pythondata = file.read().lower().replace('\n', ' ')

"""
with open('w2v/test', 'r') as file:
    pythondata = file.read().lower().replace('\n', ' ')
"""
print("Length of the training file: " + str(len(pythondata)) + ".")
print("It contains " + str(pythondata.count(" ")) + " individual code tokens.")
# Preparing the dataset (or loading already processed dataset to not do everything again)
if (os.path.isfile('data/pythontraining_processed_' + mode)):
    with open('data/pythontraining_processed_' + mode, 'rb') as fp:
        all_words = pickle.load(fp)
    print("loaded processed model.")
else:
    print("now processing...")
    processed = pythondata
    #all_words = nltk.word_tokenize(processed)
    all_words_0 = nltk.wordpunct_tokenize(processed)
    milCount = 0
    for word in all_words_0:
        temp = []
        for c in word:
            temp.append(c)

        all_words.append(temp)
        if len(all_words) / 1000000 > milCount:
            milCount += 1
            print(milCount)

    #all_words = [nltk.word_tokenize(sent) for sent in processed]
    print("saving")
    with open('data/pythontraining_processed_' + mode, 'wb') as fp:
        pickle.dump(all_words, fp)

print("processed.\n")
#exit(0)
# trying out different parameters
for mincount in [10]:  # ,30,50,100,300,500,5000]:
    print(f'Mincount: {mincount}')

    for iterationen in [50]:  # [1,5,10,30,50,100]:
        print(f'Iterations: {iterationen}')
        for s in [10]:  # [5,10,15,30,50,75,100,200,300]:
            print(f'S: {s}')
            print("\n\n" + mode + " W2V model with min count " + str(mincount) + " and " + str(
                iterationen) + " Iterationen and size " + str(s))
            fname = "c2v/char2vec_" + mode + str(mincount) + "-" + str(iterationen) + "-" + str(s) + ".model"

            if (os.path.isfile(fname)):
                print("model already exists.")
                continue

            else:
                print("calculating model...")
                # training the model
                model = Word2Vec(all_words, vector_size=s, min_count=mincount, epochs=iterationen, workers=4)
                # vocabulary = model.wv.vocab
                vocabulary = model.wv.key_to_index

                # print some examples

                # words = ["import", "true", "while", "if", "try", "in", "+", "x", "=", ":", "[", "print", "str", "count", "len", "where", "join", "split", "==", "raw_input"]
                # for similar in words:
                #  try:
                #    print("\n")
                #    print(similar)
                #    sim_words = model.wv.most_similar(similar)
                #    print(sim_words)
                #    print("\n")
                #  except Exception as e:
                #    print(e)
                #    print("\n")

                # saving the model
                model.save(fname)



