import gensim.utils
import torch

import myutils
import sys
import os.path
import json
from datetime import datetime
import random
import pickle
import numpy
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.preprocessing import sequence
from keras import backend as K
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from keras.models import load_model
from sklearn.utils import class_weight
import tensorflow as tf
from gensim.models import Word2Vec, KeyedVectors
from gensim import models
import torch.nn as nn
import transformerBlockModel
from transformerBlockModel import TokenAndPositionEmbedding
from transformerBlockModel import TransformerBlock
# for automatically saving results
auto_trace = True
trace_file = None

modelPath = None
#modelPath = 'Transformer_model_filtertest_step5_length500_edim32_numhead1_ffsz32_droput0.05_outsz1_opt_RMSprop_btsz1024_eps50_tokvocsize30000_mxl100.h5'
model = None

#default mode / type of vulnerability
mode = "sql"
# xss, remote_code_execution
#get the vulnerability from the command line argument
if (len(sys.argv) > 1):
  mode = sys.argv[1]

progress = 0
count = 0


### paramters for the filtering and creation of samples
restriction = [20000,5,6,10] #which samples to filter out
step = 5 #step lenght n in the description
fulllength = 200 #context length m in the description


embed_dim = 32  # Embedding size for each token
num_heads = 1  # Number of attention heads
ff_dim = 32  # Hidden layer size in feed forward network inside transformer
# ff_dim = [4,8,16,32,64,96,128,256,512]
dropout = 0.05
output_size = 1 # Size of final output from transformer model (prediction size, default=1)
optimizer = 'RMSprop' # SGD,RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl, Adam
batch_size = 9999
epochs = 100
tokenizer_vocab_size = 35000 # vocab size for the keras tokenizer

params = [step,fulllength,embed_dim,num_heads,ff_dim,dropout,output_size,optimizer,batch_size,epochs,tokenizer_vocab_size]
paramNames = ['step','length','edim','numhead','ffsz','droput','outsz','opt_','btsz','eps','tokvocsize']
paramsZipped = [el[0] + str(el[1]) for el in list(zip(paramNames,params))]
modelName = 'Transformer_model_filtertest_'+ '_'.join(paramsZipped)

thisDir = os.getcwd()
modelDir = 'transformers\\multimode\\models\\'
dataDir = 'separatedBlocks/'
modes = os.listdir(dataDir)

# creating the model
maxlen = 100
model = None
filters = "\t\n"
tokenizerPath = modelDir + 'keras_tokenizer.pickle'
tokenizer = None
if os.path.isfile(tokenizerPath):
    with open(tokenizerPath, 'rb') as f:
        tokenizer = pickle.load(f)
    print(f'Loaded tokenizer from {tokenizerPath}')
else:
    tokenizer = keras.preprocessing.text.Tokenizer(num_words=tokenizer_vocab_size, filters=filters)


if modelPath != None and os.path.exists(modelPath):
    custom_objects = {"TokenAndPositionEmbedding": TokenAndPositionEmbedding, "TransformerBlock": TransformerBlock,
                      'f1_loss': myutils.f1_loss, 'f1': myutils.f1}
    print(f'Loading model: {modelDir+modelPath}\n')
    model = load_model(modelDir+modelPath,custom_objects=custom_objects)
else:
    model = keras.Sequential()
    model.add(layers.Input(shape=(maxlen, )))
    model.add(TokenAndPositionEmbedding(maxlen, tokenizer_vocab_size, embed_dim))
    model.add(TransformerBlock(embed_dim, num_heads, ff_dim))
    model.add(layers.GlobalAveragePooling1D())
    model.add(layers.Dropout(dropout))
    model.add(layers.Dense(ff_dim, activation='relu'))
    model.add(layers.Dropout(dropout))
    model.add(layers.Dense(output_size, activation='sigmoid'))
#config = model.get_config()

#model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    model.compile(loss=myutils.f1_loss, optimizer=optimizer, metrics=[myutils.f1])

modes = ['xss','remote_code_execution','xsrf','sql','open_redirect','path_disclosure','command_injection']
slices = [1,2,3,4,5,6,7]#,3,4,5,6,7]
validationDone = False
testDone = False
ValidateX = []
ValidateY = []
TrainX = []
TrainY = []
TestX = {mode:[] for mode in modes}
TestY = {mode:[] for mode in modes}
testResults = {mode:{'F1':-13,'Accuracy':-13,'Recall':-13,'Precision':-13} for mode in modes}

for thisSlice in slices:

    for mode in modes:
        thisDataDir = dataDir + mode
        thisTrainPath = thisDataDir + '/' + f'{mode}_processed_step{step}_length{fulllength}_train_{thisSlice}'
        thisTrainLabelPath = thisDataDir + '/' + f'{mode}_processed_step{step}_length{fulllength}_train_labels_{thisSlice}'

        thisValidatePath = thisDataDir + '/' + f'{mode}_processed_step{step}_length{fulllength}_validate'
        thisValidateLabelsPath = thisDataDir + '/' + f'{mode}_processed_step{step}_length{fulllength}_validate_labels'

        thisTestPath = thisDataDir + '/' + f'{mode}_processed_step{step}_length{fulllength}_test'
        thisTestLabelsPath = thisDataDir + '/' + f'{mode}_processed_step{step}_length{fulllength}_test_labels'

        with open(thisTrainPath, 'rb') as fp:
            thisTrainX = pickle.load(fp)
            TrainX.extend(thisTrainX)
        with open(thisTrainLabelPath, 'rb') as fp:
            thisTrainY = pickle.load(fp)
            TrainY.extend(thisTrainY)

        if not validationDone:
            with open(thisValidatePath, 'rb') as fp:
                ValidateX.extend(pickle.load(fp))
            with open(thisValidateLabelsPath, 'rb') as fp:
                ValidateY.extend(pickle.load(fp))

        #if not testDone:
        with open(thisTestPath, 'rb') as fp:
            TestX[mode].extend(pickle.load(fp))
        with open(thisTestLabelsPath, 'rb') as fp:
            TestY[mode].extend(pickle.load(fp))
    validationDone = True
    #testDone = True

    """   
    with open(dataDir + mode + f'_processed_step{step}_length{fulllength}_train', 'rb') as fp:
        TrainX = pickle.load(fp)
    with open(dataDir + mode + f'_processed_step{step}_length{fulllength}_train_labels', 'rb') as fp:
        TrainY = pickle.load(fp)
    """

""" shuffling train and validate data """

trainIndices = [i for i in range(len(TrainX))]
validateIndices = [i for i in range(len(ValidateX))]
testIndices = [i for i in range(len(TestX))]
random.shuffle(trainIndices)
random.shuffle(validateIndices)
random.shuffle(testIndices)

TrainXnew = [TrainX[i] for i in trainIndices]
TrainYnew = [TrainY[i] for i in trainIndices]
ValidateXnew = [ValidateX[i] for i in validateIndices]
ValidateYnew = [ValidateY[i] for i in validateIndices]

"""
TrainXnew = TrainX
TrainYnew = TrainY
ValidateXnew = ValidateX
ValidateYnew = ValidateY
"""
print("Train length: " + str(len(TrainXnew)))
print("Validation length: " + str(len(ValidateXnew)))
now = datetime.now() # current date and time
nowformat = now.strftime("%H:%M")
print("time: ", nowformat)

tokenizer.fit_on_texts(TrainXnew)
tokenizer.fit_on_texts(ValidateXnew)
#tokenizer.fit_on_texts(TestXnew)

train_text_to_int = tokenizer.texts_to_sequences(TrainXnew)
#if not validationDone:
val_texts_to_int = tokenizer.texts_to_sequences(ValidateXnew)
#validationDone = True
#maxlen = len(train_text_to_int[0]) 
for i in range(len(train_text_to_int)):
    if len(train_text_to_int[i]) > maxlen: maxlen = len(train_text_to_int[i])

for i in range(len(val_texts_to_int)):
    if len(val_texts_to_int[i]) > maxlen: maxlen = len(val_texts_to_int[i])

print(f'Maxlen has been set to {maxlen}')

## pad sequences
padding_type = 'post'
X_train = keras.preprocessing.sequence.pad_sequences(train_text_to_int,
                                                              maxlen=maxlen,
                                                              truncating='pre',
                                                              padding=padding_type)

y_train = numpy.array(TrainYnew)
X_test = keras.preprocessing.sequence.pad_sequences(val_texts_to_int,
                                                              maxlen=maxlen,
                                                              truncating='pre',
                                                              padding=padding_type)
y_test =  numpy.array(ValidateYnew)



#in the original collection of data, the 0 and 1 were used the other way round, so now they are switched so that "1" means vulnerable and "0" means clean.

for i in range(len(y_train)):
  y_train[i] = 1 - y_train[i]

for i in range(len(y_test)):
  y_test[i] = 1 - y_test[i]

now = datetime.now() # current date and time
nowformat = now.strftime("%H:%M")
print("numpy array done. ", nowformat)

print(str(len(X_train)) + " samples in the training set.")
print(str(len(X_test)) + " samples in the validation set.")


csum = 0
for a in y_train:
  csum = csum+a
print("percentage of vulnerable samples: "  + str(int((csum / len(X_train)) * 10000)/100) + "%")

testvul = 0
for y in y_test:
  if y == 1:
    testvul = testvul+1
print("absolute amount of vulnerable samples in test set: " + str(testvul))

now = datetime.now() # current date and time
nowformat = now.strftime("%H:%M")
print("Starting Transformer: ", nowformat)

history = model.fit(X_train.astype(float), y_train.astype(float), batch_size=batch_size, epochs=epochs)

#history = model.fit(X_train.astype(float), y_train.astype(float), batch_size=2048, epochs=100, validation_data=(X_test.astype(float), y_test.astype(float)))

now = datetime.now() # current date and time
nowformat = now.strftime("%H:%M")
print("Compiled Transformer: ", nowformat)


if auto_trace:
    traceDir = 'transformers/multimode/traces/'
    if not os.path.isdir(traceDir):
        os.mkdir(traceDir)
    trace_file = open(traceDir + '/' + modelName + '.txt', 'w')

#validate data on train and test set

for dataset in ["train","validate"]:
    print("Now predicting on " + dataset + " set ")

    ## placeholder values for variable declaration
    accuracy = -13
    precision = -13
    recall = -13
    F1Score = -13

    if dataset == "train":
        yhat_classes = (model.predict(X_train) > 0.5).astype("int32")
        accuracy = accuracy_score(y_train, yhat_classes)
        precision = precision_score(y_train, yhat_classes)
        recall = recall_score(y_train, yhat_classes)
        F1Score = f1_score(y_train, yhat_classes)

    if dataset == "validate":
        yhat_classes = (model.predict(X_test) > 0.5).astype("int32")
        accuracy = accuracy_score(y_test, yhat_classes)
        precision = precision_score(y_test, yhat_classes)
        recall = recall_score(y_test, yhat_classes)
        F1Score = f1_score(y_test, yhat_classes)

    thisPrint = '\n'.join([
      "Accuracy: " + str(accuracy),
      "Precision: " + str(precision),
      "Recall: " + str(recall),
      'F1 score: %f' % F1Score,
      "\n"
    ])
    print(thisPrint)
    trace_file.write(thisPrint)

for mode in modes:
    print(f'Now testing on dataset {mode}')

    ## placeholder values for variable declaration
    accuracy = -13
    precision = -13
    recall = -13
    F1Score = -13

    thisX = TestX[mode]
    thisY = TestY[mode]
    testIndices = [i for i in range(len(thisX))]
    random.shuffle(testIndices)
    TestXnew = [thisX[i] for i in testIndices]
    TestYnew = [thisY[i] for i in testIndices]

    test_texts_to_int = tokenizer.texts_to_sequences(TestXnew)
    X_test_final = keras.preprocessing.sequence.pad_sequences(test_texts_to_int,
                                                              maxlen=maxlen,
                                                              truncating='pre',
                                                              padding=padding_type)
    y_test_final = numpy.array(TestYnew)


    print(str(len(X_test_final)) + f" samples in {mode} test set.")

    for i in range(len(y_test_final)):
        y_test_final[i] = 1 - y_test_final[i]

    finaltestvul = 0
    for y in y_test_final:
        if y == 1:
            finaltestvul = finaltestvul + 1
    print(f"absolute amount of vulnerable samples in final test set  of {mode}: " + str(finaltestvul))

    yhat_classes = (model.predict(X_test_final) > 0.5).astype("int32")
    accuracy = accuracy_score(y_test_final, yhat_classes)
    precision = precision_score(y_test_final, yhat_classes)
    recall = recall_score(y_test_final, yhat_classes)
    F1Score = f1_score(y_test_final, yhat_classes)

    thisPrint = '\n'.join([
        "Accuracy: " + str(accuracy),
        "Precision: " + str(precision),
        "Recall: " + str(recall),
        'F1 score: %f' % F1Score,
        "\n"
    ])
    testResults[mode]['Accuracy'] = accuracy
    testResults[mode]['Precision'] = precision
    testResults[mode]['Recall'] = recall
    testResults[mode]['F1'] = F1Score

    print(thisPrint)
    trace_file.write(thisPrint)

finalPrint = '\n'.join([
    "Accuracy: " + str( sum( [testResults[el]['Accuracy'] for el in modes ] ) / len(modes) ),
    "Precision: " + str( sum( [testResults[el]['Precision'] for el in modes ] ) / len(modes) ),
    "Recall: " + str( sum( [testResults[el]['Recall'] for el in modes ] ) / len(modes) ),
    'F1 score:' + str( sum( [testResults[el]['F1'] for el in modes ] ) / len(modes) ),
    "\n"
])

if auto_trace:
    trace_file.close()

#optimizerState = model.optimizer
now = datetime.now() # current date and time
nowformat = now.strftime("%H:%M")

"""
tokenizerName = 'keras_tokenizer.pickle'
print(f"saving tokenizer at {modelDir + tokenizerName}")
with open(modelDir + tokenizerName,'wb') as f:
    pickle.dump(tokenizer,f,protocol=pickle.HIGHEST_PROTOCOL)
"""
print("\n\n")

modelName = modelName + '_mxl' + str(maxlen) + '.h5'
print("saving transformer model as " + modelName + ". ", nowformat)
model.save(modelDir + modelName)

