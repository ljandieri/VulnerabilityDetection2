C:\Python39\python.exe "C:\Program Files\JetBrains\PyCharm Community Edition 2021.2.3\plugins\python-ce\helpers\pydev\pydevd.py" --multiproc --qt-support=auto --client 127.0.0.1 --port 50889 --file D:/VulnerabilityDetection3/Code/c2v_makemodel.py
Connected to pydev debugger (build 212.5457.59)
finished loading.  11:26
cutoff 67255
cutoff2 81666
Creating training dataset... (remote_code_execution)
Creating validation dataset...
Creating finaltest dataset...
Train length: 67255
Test length: 14411
Finaltesting length: 14412
time:  11:34
D:/VulnerabilityDetection3/Code/c2v_makemodel.py:257: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X_train =  numpy.array(TrainX)
D:/VulnerabilityDetection3/Code/c2v_makemodel.py:263: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X_test =  numpy.array(ValidateX)
D:/VulnerabilityDetection3/Code/c2v_makemodel.py:265: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X_finaltest =  numpy.array(FinaltestX)
numpy array done.  11:34
67255 samples in the training set.
14411 samples in the validation set.
14412 samples in the final test set.
percentage of vulnerable samples: 8.86%
absolute amount of vulnerable samples in test set: 1296
Starting LSTM:  11:34
Dropout: 0.2
Neurons: 50
Optimizer: adam
Epochs: 20
Batch Size: 32
max length: 200
2022-03-08 11:35:54.906606: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-03-08 11:35:55.986830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3969 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5
Compiled LSTM:  11:35
2022-03-08 11:36:02.968748: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1076080000 exceeds 10% of free system memory.
Epoch 1/20
2022-03-08 11:36:09.107089: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8200
2102/2102 [==============================] - 28s 10ms/step - loss: 0.8207 - f1: 0.1579
Epoch 2/20
2102/2102 [==============================] - 21s 10ms/step - loss: 0.8145 - f1: 0.1655
Epoch 3/20
2102/2102 [==============================] - 22s 10ms/step - loss: 0.7728 - f1: 0.2063
Epoch 4/20
2102/2102 [==============================] - 21s 10ms/step - loss: 0.7444 - f1: 0.2363
Epoch 5/20
2102/2102 [==============================] - 21s 10ms/step - loss: 0.7389 - f1: 0.2404
Epoch 6/20
2102/2102 [==============================] - 23s 11ms/step - loss: 0.7214 - f1: 0.2523
Epoch 7/20
2102/2102 [==============================] - 22s 11ms/step - loss: 0.6945 - f1: 0.2809
Epoch 8/20
2102/2102 [==============================] - 23s 11ms/step - loss: 0.6779 - f1: 0.2967
Epoch 9/20
2102/2102 [==============================] - 23s 11ms/step - loss: 0.6639 - f1: 0.3139
Epoch 10/20
2102/2102 [==============================] - 23s 11ms/step - loss: 0.6577 - f1: 0.3204
Epoch 11/20
2102/2102 [==============================] - 21s 10ms/step - loss: 0.6517 - f1: 0.3246
Epoch 12/20
2102/2102 [==============================] - 22s 10ms/step - loss: 0.6480 - f1: 0.3293
Epoch 13/20
2102/2102 [==============================] - 21s 10ms/step - loss: 0.6246 - f1: 0.3526
Epoch 14/20
2102/2102 [==============================] - 23s 11ms/step - loss: 0.6060 - f1: 0.3705
Epoch 15/20
2102/2102 [==============================] - 22s 11ms/step - loss: 0.6048 - f1: 0.3733
Epoch 16/20
2102/2102 [==============================] - 22s 11ms/step - loss: 0.6385 - f1: 0.3383
Epoch 17/20
2102/2102 [==============================] - 21s 10ms/step - loss: 0.5919 - f1: 0.3812
Epoch 18/20
2102/2102 [==============================] - 21s 10ms/step - loss: 0.5627 - f1: 0.4159
Epoch 19/20
2102/2102 [==============================] - 22s 10ms/step - loss: 0.5586 - f1: 0.4177
Epoch 20/20
2102/2102 [==============================] - 22s 10ms/step - loss: 0.5467 - f1: 0.4246
Now predicting on train set (0.2 dropout)
Accuracy: 0.8989963571481674
Precision: 0.4517513337972628
Recall: 0.653413856735447
F1 score: 0.534184
Now predicting on test set (0.2 dropout)
Accuracy: 0.8976476302824231
Precision: 0.45277044854881265
Recall: 0.6620370370370371
F1 score: 0.537762
Now predicting on finaltest set (0.2 dropout)
Accuracy: 0.891548709408826
Precision: 0.4391644908616188
Recall: 0.6323308270676692
F1 score: 0.518336
saving LSTM model remote_code_execution.  11:43