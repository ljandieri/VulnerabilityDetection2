import sys
import pickle
import os
import json
from datetime import datetime
import myutils
import random

modes = ['xss','xsrf','sql','remote_code_execution','path_disclosure','open_redirect','command_injection']
progress = 0
count = 0

### paramters for the filtering and creation of samples
#restriction = [20000, 5, 6, 10]  # which samples to filter out
step = 5  # step lenght n in the description
fulllength = 500  # context length m in the description

mode2 = str(step) + "_" + str(fulllength)

thisDir = os.getcwd()
targetRelativePath = '\\aggragated_data\\'
targetAbsPath = thisDir + targetRelativePath
targetFileName =  f'_aggragated_step{step}_length{fulllength}_' + '-'.join(modes)
allmodeblocks = []
allmodeslen = 0

for mode in modes:
    thisDataPath = thisDir + '\\' + 'data/plain_' + mode
    data = None
    with open(thisDataPath, 'r') as infile:
        data = json.load(infile)
    now = datetime.now()  # current date and time
    nowformat = now.strftime("%H:%M")
    print(f"finished loading data from {mode}. ", nowformat)

    allblocks = []

    for r in data:
        progress = progress + 1

        for c in data[r]:

            if "files" in data[r][c]:
                #  if len(data[r][c]["files"]) > restriction[3]:
                # too many files
                #    continue

                for f in data[r][c]["files"]:

                    #      if len(data[r][c]["files"][f]["changes"]) >= restriction[2]:
                    # too many changes in a single file
                    #       continue

                    if not "source" in data[r][c]["files"][f]:
                        # no sourcecode
                        continue

                    if "source" in data[r][c]["files"][f]:
                        sourcecode = data[r][c]["files"][f]["source"]
                        #     if len(sourcecode) > restriction[0]:
                        # sourcecode is too long
                        #       continue

                        allbadparts = []

                        for change in data[r][c]["files"][f]["changes"]:

                            # get the modified or removed parts from each change that happened in the commit
                            badparts = change["badparts"]
                            count = count + len(badparts)

                            #     if len(badparts) > restriction[1]:
                            # too many modifications in one change
                            #       break

                            for bad in badparts:
                                # check if they can be found within the file
                                pos = myutils.findposition(bad, sourcecode)
                                if not -1 in pos:
                                    allbadparts.append(bad)

                        #   if (len(allbadparts) > restriction[2]):
                        # too many bad positions in the file
                        #     break

                        if (len(allbadparts) > 0):
                            #   if len(allbadparts) < restriction[2]:
                            # find the positions of all modified parts
                            positions = myutils.findpositions(allbadparts, sourcecode)

                            # get the file split up in samples
                            blocks = myutils.getblocks(sourcecode, positions, step, fulllength)

                            for b in blocks:
                                # each is a tuple of code and label
                                allblocks.append(b)


    allblockssize = sys.getsizeof(allblocks)

    print(f"total length of the sample set {mode}: " + str(len(allblocks)))
    allmodeblocks.extend(allblocks)
    allmodeslen += len(allblocks)

keys = []
for i in range(len(allmodeblocks)):
  keys.append(i)
random.shuffle(keys)

cutoff = round(0.7 * len(keys)) #     70% for the training set
cutoff2 = round(0.85 * len(keys)) #   15% for the validation set and 15% for the final test set

keystrain = keys[:cutoff]
keysvalidate = keys[cutoff:cutoff2]
keystest = keys[cutoff2:]

print("cutoff " + str(cutoff))
print("cutoff2 " + str(cutoff2))

TrainX = []
TrainY = []
ValidateX = []
ValidateY = []
TestX = []
TestY = []

print("Splitting dataset...")

for k in keystrain:
  block = allmodeblocks[k]
  code = block[0]
  TrainX.append(code)
  TrainY.append(block[1]) #append the label to the Y (dependent variable)

print("Creating validation dataset...")
for k in keysvalidate:
  block = allmodeblocks[k]
  code = block[0]
  ValidateX.append(code)
  ValidateY.append(block[1]) #append the label to the Y (dependent variable)

print("Creating finaltest dataset...")
for k in keystest:
  block = allmodeblocks[k]
  code = block[0]
  TestX.append(code)
  TestY.append(block[1]) #append the label to the Y (dependent variable)

print("Train length: " + str(len(TrainX)))
print("Test length: " + str(len(ValidateX)))
print("Finaltesting length: " + str(len(TestX)))

with open(targetAbsPath + targetFileName + '_train', 'wb') as fp:
  pickle.dump(TrainX, fp)

with open(targetAbsPath + targetFileName + '_validate', 'wb') as fp:
  pickle.dump(ValidateX, fp)

with open(targetAbsPath + targetFileName + '_test', 'wb') as fp:
  pickle.dump(TestX, fp)

with open(targetAbsPath + targetFileName + '_train_labels', 'wb') as fp:
  pickle.dump(TrainY, fp)

with open(targetAbsPath + targetFileName + '_validate_labels', 'wb') as fp:
  pickle.dump(ValidateY, fp)

with open(targetAbsPath + targetFileName + '_test_labels', 'wb') as fp:
  pickle.dump(TestY, fp)

print(f'all blocks successfully saved')
